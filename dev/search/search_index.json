{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"[ ] report-card Kube-bench kube-bench is a Go application that checks whether Kubernetes is deployed securely by running the checks documented in the CIS Kubernetes Benchmark . Tests are configured with YAML files, making this tool easy to update as test specifications evolve. kube-bench implements the CIS Kubernetes Benchmark as closely as possible. Please raise issues here if kube-bench is not correctly implementing the test as described in the Benchmark. To report issues in the Benchmark itself (for example, tests that you believe are inappropriate), please join the CIS community . There is not a one-to-one mapping between releases of Kubernetes and releases of the CIS benchmark. See CIS Kubernetes Benchmark support to see which releases of Kubernetes are covered by different releases of the benchmark. It is impossible to inspect the master nodes of managed clusters, e.g. GKE, EKS, AKS and ACK, using kube-bench as one does not have access to such nodes, although it is still possible to use kube-bench to check worker node configuration in these environments. For help and more information go to our github discussions q&a","title":"Overview"},{"location":"#kube-bench","text":"kube-bench is a Go application that checks whether Kubernetes is deployed securely by running the checks documented in the CIS Kubernetes Benchmark . Tests are configured with YAML files, making this tool easy to update as test specifications evolve. kube-bench implements the CIS Kubernetes Benchmark as closely as possible. Please raise issues here if kube-bench is not correctly implementing the test as described in the Benchmark. To report issues in the Benchmark itself (for example, tests that you believe are inappropriate), please join the CIS community . There is not a one-to-one mapping between releases of Kubernetes and releases of the CIS benchmark. See CIS Kubernetes Benchmark support to see which releases of Kubernetes are covered by different releases of the benchmark. It is impossible to inspect the master nodes of managed clusters, e.g. GKE, EKS, AKS and ACK, using kube-bench as one does not have access to such nodes, although it is still possible to use kube-bench to check worker node configuration in these environments. For help and more information go to our github discussions q&a","title":"Kube-bench"},{"location":"architecture/","text":"Test config YAML representation The tests (or \"controls\") are maintained in YAML documents. There are different versions of these test YAML files reflecting different versions and platforms of the CIS Kubernetes Benchmark . You will find more information about the test file YAML definitions in our controls documentation . Kube-bench benchmarks The test files for the various versions of Benchmarks can be found in directories with same name as the Benchmark versions under the cfg directory next to the kube-bench executable, for example ./cfg/cis-1.5 will contain all test files for CIS Kubernetes Benchmark v1.5.1 which are: master.yaml, controlplane.yaml, node.yaml, etcd.yaml, policies.yaml and config.yaml Check the contents of the benchmark directory under cfg to see which targets are available for that benchmark. Each file except config.yaml represents a target (also known as a control in other parts of this documentation). The following table shows the valid targets based on the CIS Benchmark version. | CIS Benchmark | Targets | |---|---| | cis-1.5| master, controlplane, node, etcd, policies | | cis-1.6| master, controlplane, node, etcd, policies | | gke-1.0| master, controlplane, node, etcd, policies, managedservices | | eks-1.0| controlplane, node, policies, managedservices | | ack-1.0| master, controlplane, node, etcd, policies, managedservices | | rh-0.7| master,node| | rh-1.0| master, controlplane, node, etcd, policies |","title":"Architecture"},{"location":"architecture/#test-config-yaml-representation","text":"The tests (or \"controls\") are maintained in YAML documents. There are different versions of these test YAML files reflecting different versions and platforms of the CIS Kubernetes Benchmark . You will find more information about the test file YAML definitions in our controls documentation .","title":"Test config YAML representation"},{"location":"architecture/#kube-bench-benchmarks","text":"The test files for the various versions of Benchmarks can be found in directories with same name as the Benchmark versions under the cfg directory next to the kube-bench executable, for example ./cfg/cis-1.5 will contain all test files for CIS Kubernetes Benchmark v1.5.1 which are: master.yaml, controlplane.yaml, node.yaml, etcd.yaml, policies.yaml and config.yaml Check the contents of the benchmark directory under cfg to see which targets are available for that benchmark. Each file except config.yaml represents a target (also known as a control in other parts of this documentation). The following table shows the valid targets based on the CIS Benchmark version. | CIS Benchmark | Targets | |---|---| | cis-1.5| master, controlplane, node, etcd, policies | | cis-1.6| master, controlplane, node, etcd, policies | | gke-1.0| master, controlplane, node, etcd, policies, managedservices | | eks-1.0| controlplane, node, policies, managedservices | | ack-1.0| master, controlplane, node, etcd, policies, managedservices | | rh-0.7| master,node| | rh-1.0| master, controlplane, node, etcd, policies |","title":"Kube-bench benchmarks"},{"location":"asff/","text":"Integrating kube-bench with AWS Security Hub You can configure kube-bench with the --asff to send findings to AWS Security Hub. There are some additional steps required so that kube-bench has information and permissions to send these findings. Enable the AWS Security Hub integration You will need AWS Security Hub to be enabled in your account In the Security Hub console, under Integrations, search for kube-bench Click on Accept findings . This gives information about the IAM permissions required to send findings to your Security Hub account. kube-bench runs within a pod on your EKS cluster, and will need to be associated with a Role that has these permissions. Configure permissions in an IAM Role Grant these permissions to the IAM Role that the kube-bench pod will be associated with. There are two options: You can run the kube-bench pod under a specific service account associated with an IAM role that has these permissions to write Security Hub findings. Alternatively the pod can be granted permissions specified by the Role that your EKS node group uses . Here is an example IAM Policy that you can attach to your EKS node group's IAM Role: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"securityhub:BatchImportFindings\" , \"Resource\" : [ \"arn:aws:securityhub:us-east-1::product/aqua-security/kube-bench\" ] } ] } Modify the job configuration Modify the kube-bench Configmap in job-eks-asff.yaml to specify the AWS account, AWS region, and the EKS Cluster ARN. In the same file, modify the image specifed in the Job to use the kube-bench image pushed to your ECR [Optional] - If you have created a dedicated IAM role to be used with kube-bench as described above in Configure permissions in an IAM Role , you will need to add the IAM role arn to the kube-bench ServiceAccount in job-eks-asff.yaml . Make sure that job-eks-asff.yaml specifies the container image you just pushed to your ECR registry. You can now run kube-bench as a pod in your cluster: kubectl apply -f job-eks-asff.yaml Findings will be generated for any kube-bench test that generates a [FAIL] or [WARN] output. If all tests pass, no findings will be generated. However, it's recommended that you consult the pod log output to check whether any findings were generated but could not be written to Security Hub.","title":"ASFF"},{"location":"asff/#integrating-kube-bench-with-aws-security-hub","text":"You can configure kube-bench with the --asff to send findings to AWS Security Hub. There are some additional steps required so that kube-bench has information and permissions to send these findings.","title":"Integrating kube-bench with AWS Security Hub"},{"location":"asff/#enable-the-aws-security-hub-integration","text":"You will need AWS Security Hub to be enabled in your account In the Security Hub console, under Integrations, search for kube-bench Click on Accept findings . This gives information about the IAM permissions required to send findings to your Security Hub account. kube-bench runs within a pod on your EKS cluster, and will need to be associated with a Role that has these permissions.","title":"Enable the AWS Security Hub integration"},{"location":"asff/#configure-permissions-in-an-iam-role","text":"Grant these permissions to the IAM Role that the kube-bench pod will be associated with. There are two options: You can run the kube-bench pod under a specific service account associated with an IAM role that has these permissions to write Security Hub findings. Alternatively the pod can be granted permissions specified by the Role that your EKS node group uses . Here is an example IAM Policy that you can attach to your EKS node group's IAM Role: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"securityhub:BatchImportFindings\" , \"Resource\" : [ \"arn:aws:securityhub:us-east-1::product/aqua-security/kube-bench\" ] } ] }","title":"Configure permissions in an IAM Role"},{"location":"asff/#modify-the-job-configuration","text":"Modify the kube-bench Configmap in job-eks-asff.yaml to specify the AWS account, AWS region, and the EKS Cluster ARN. In the same file, modify the image specifed in the Job to use the kube-bench image pushed to your ECR [Optional] - If you have created a dedicated IAM role to be used with kube-bench as described above in Configure permissions in an IAM Role , you will need to add the IAM role arn to the kube-bench ServiceAccount in job-eks-asff.yaml . Make sure that job-eks-asff.yaml specifies the container image you just pushed to your ECR registry. You can now run kube-bench as a pod in your cluster: kubectl apply -f job-eks-asff.yaml Findings will be generated for any kube-bench test that generates a [FAIL] or [WARN] output. If all tests pass, no findings will be generated. However, it's recommended that you consult the pod log output to check whether any findings were generated but could not be written to Security Hub.","title":"Modify the job configuration"},{"location":"controls/","text":"Test and config files kube-bench runs checks specified in controls files that are a YAML representation of the CIS Kubernetes Benchmark checks (or other distribution-specific hardening guides). Controls controls is a YAML document that contains checks that must be run against a specific Kubernetes node type, master or node and version. controls is the fundamental input to kube-bench . The following is an example of a basic controls : --- controls: id: 1 text: \"Master Node Security Configuration\" type: \"master\" groups: - id: 1.1 text: API Server checks: - id: 1.1.1 text: \"Ensure that the --allow-privileged argument is set (Scored)\" audit: \"ps -ef | grep kube-apiserver | grep -v grep\" tests: bin_op: or test_items: - flag: \"--allow-privileged\" set: true - flag: \"--some-other-flag\" set: false remediation: \"Edit the /etc/kubernetes/config file on the master node and set the KUBE_ALLOW_PRIV parameter to '--allow-privileged=false'\" scored: true - id: 1.2 text: Scheduler checks: - id: 1.2.1 text: \"Ensure that the --profiling argument is set to false (Scored)\" audit: \"ps -ef | grep kube-scheduler | grep -v grep\" tests: bin_op: and test_items: - flag: \"--profiling\" set: true - flag: \"--some-other-flag\" set: false remediation: \"Edit the /etc/kubernetes/config file on the master node and set the KUBE_ALLOW_PRIV parameter to '--allow-privileged=false'\" scored: true controls is composed of a hierarchy of groups, sub-groups and checks. Each of the controls components have an id and a text description which are displayed in the kube-bench output. type specifies what kubernetes node type a controls is for. Possible values for type are master and node . Groups groups is a list of subgroups that test the various Kubernetes components that run on the node type specified in the controls . For example, one subgroup checks parameters passed to the API server binary, while another subgroup checks parameters passed to the controller-manager binary. groups: - id: 1.1 text: API Server # ... - id: 1.2 text: Scheduler # ... These subgroups have id , text fields which serve the same purposes described in the previous paragraphs. The most important part of the subgroup is the checks field which is the collection of actual check s that form the subgroup. This is an example of a subgroup and checks in the subgroup. id: 1.1 text: API Server checks: - id: 1.1.1 text: \"Ensure that the --allow-privileged argument is set (Scored)\" audit: \"ps -ef | grep kube-apiserver | grep -v grep\" tests: # ... - id: 1.1.2 text: \"Ensure that the --anonymous-auth argument is set to false (Not Scored)\" audit: \"ps -ef | grep kube-apiserver | grep -v grep\" tests: # ... kube-bench supports running a subgroup by specifying the subgroup id on the command line, with the flag --group or -g . Check The CIS Kubernetes Benchmark recommends configurations to harden Kubernetes components. These recommendations are usually configuration options and can be specified by flags to Kubernetes binaries, or in configuration files. The Benchmark also provides commands to audit a Kubernetes installation, identify places where the cluster security can be improved, and steps to remediate these identified problems. In kube-bench , check objects embody these recommendations. This an example check object: id: 1.1.1 text: \"Ensure that the --anonymous-auth argument is set to false (Not Scored)\" audit: \"ps -ef | grep kube-apiserver | grep -v grep\" tests: test_items: - flag: \"--anonymous-auth\" compare: op: eq value: false set: true remediation: | Edit the API server pod specification file kube-apiserver on the master node and set the below parameter. --anonymous-auth=false scored: false A check object has an id , a text , an audit , a tests , remediation and scored fields. kube-bench supports running individual checks by specifying the check's id as a comma-delimited list on the command line with the --check flag. The audit field specifies the command to run for a check. The output of this command is then evaluated for conformance with the CIS Kubernetes Benchmark recommendation. The audit is evaluated against criteria specified by the tests object. tests contain bin_op and test_items . test_items specify the criteria(s) the audit command's output should meet to pass a check. This criteria is made up of keywords extracted from the output of the audit command and operations that compare these keywords against values expected by the CIS Kubernetes Benchmark. There are three ways to run and extract keywords from the output of the command used, | Command | Output var | |---|---| | audit | flag | | audit_config | path | | audit_env | env | flag is used when the keyword is a command-line flag. The associated audit command could be any binaries available on the system like ps command and a grep for the binary whose flag we are checking: ps -ef | grep somebinary | grep -v grep Here is an example usage of the flag option: # ... audit: \"ps -ef | grep kube-apiserver | grep -v grep\" tests: test_items: - flag: \"--anonymous-auth\" # ... path is used when the keyword is an option set in a JSON or YAML config file. The associated audit_command command is usually cat /path/to/config-yaml-or-json . For example: # ... text: \"Ensure that the --anonymous-auth argument is set to false (Not Scored)\" audit: \"cat /path/to/some/config\" tests: test_items: - path: \"{.someoption.value}\" # ... env is used to check if the value is present within a specified environment variable. The presence of env is treated as an OR operation, if both flag and env are supplied it will use either to attempt pass the check. The command used for checking the environment variables of a process is generated by default . If the command being generated is causing errors, you can override the command used by setting audit_env on the check. Similarly, if you don't want the environment checking command to be generated or run at all, specify disableEnvTesting as true on the check. The example below will check if the flag --auto-tls is equal to false OR ETCD_AUTO_TLS is equal to false test_items: - flag: \"--auto-tls\" env: \"ETCD_AUTO_TLS\" compare: op: eq value: false Note: flag, path and env will act as OR if more then one present. test_item compares the output of the audit command and keywords using the set and compare fields. test_items: - flag: \"--anonymous-auth\" compare: op: eq value: false set: true set checks if a keyword is present in the output of the audit command or a config file. The possible values for set are true and false. If set is true, the check passes only if the keyword is present in the output of the audit command, or config file. If set is false, the check passes only if the keyword is not present in the output of the audit command, or config file. set is true by default. compare has two fields op and value to compare keywords with expected value. op specifies which operation is used for the comparison, and value specifies the value to compare against. To use compare , set must true. The comparison will be ignored if set is false The op (operations) currently supported in kube-bench are: - eq : tests if the keyword is equal to the compared value. - noteq : tests if the keyword is unequal to the compared value. - gt : tests if the keyword is greater than the compared value. - gte : tests if the keyword is greater than or equal to the compared value. - lt : tests if the keyword is less than the compared value. - lte : tests if the keyword is less than or equal to the compared value. - has : tests if the keyword contains the compared value. - nothave : tests if the keyword does not contain the compared value. - regex : tests if the flag value matches the compared value regular expression. When defining regular expressions in YAML it is generally easier to wrap them in single quotes, for example '^[abc]$' , to avoid issues with string escaping. - bitmask : tests if keyward is bitmasked with the compared value, common usege is for comparing file permissions in linux. Omitting checks If you decide that a recommendation is not appropriate for your environment, you can choose to omit it by editing the test YAML file to give it the check type skip as in this example: checks : - id : 2.1.1 text : \"Ensure that the --allow-privileged argument is set to false (Scored)\" type : \"skip\" scored : true No tests will be run for this check and the output will be marked [INFO]. Configuration and Variables Kubernetes component configuration and binary file locations and names vary based on cluster deployment methods and Kubernetes distribution used. For this reason, the locations of these binaries and config files are configurable by editing the cfg/config.yaml file and these binaries and files can be referenced in a controls file via variables. The cfg/config.yaml file is a global configuration file. Configuration files can be created for specific Kubernetes versions (distributions). Values in the version-specific config overwrite similar values in cfg/config.yaml . For example, the kube-apiserver in Red Hat OCP distribution is run as hypershift openshift-kube-apiserver instead of the default kube-apiserver . This difference can be specified by editing the master.apiserver.defaultbin entry cfg/rh-0.7/config.yaml . Below is the structure of cfg/config.yaml : nodetype |-- components |-- component1 |-- component1 |-- bins |-- defaultbin (optional) |-- confs |-- defaultconf (optional) |-- svcs |-- defaultsvc (optional) |-- kubeconfig |-- defaultkubeconfig (optional) Every node type has a subsection that specifies the main configuration items. components : A list of components for the node type. For example master will have an entry for apiserver , scheduler and controllermanager . Each component has the following entries: bins : A list of candidate binaries for a component. kube-bench checks this list and selects the first binary that is running on the node. If none of the binaries in bins list is running, kube-bench checks if the binary specified by defaultbin is running and terminates if none of the binaries in both bins and defaultbin is running. The selected binary for a component can be referenced in controls using a variable in the form $<component>bin . In the example below, we reference the selected API server binary with the variable $apiserverbin in an audit command. id: 1.1.1 text: \"Ensure that the --anonymous-auth argument is set to false (Scored)\" audit: \"ps -ef | grep $apiserverbin | grep -v grep\" # ... confs : A list of candidate configuration files for a component. kube-bench checks this list and selects the first config file that is found on the node. If none of the config files exists, kube-bench defaults conf to the value of defaultconf . The selected config for a component can be referenced in controls using a variable in the form $<component>conf . In the example below, we reference the selected API server config file with the variable $apiserverconf in an audit command. id: 1.4.1 text: \"Ensure that the API server pod specification file permissions are set to 644 or more restrictive (Scored)\" audit: \"/bin/sh -c 'if test -e $apiserverconf; then stat -c %a $apiserverconf; fi'\" svcs : A list of candidate unitfiles for a component. kube-bench checks this list and selects the first unitfile that is found on the node. If none of the unitfiles exists, kube-bench defaults unitfile to the value of defaultsvc . The selected unitfile for a component can be referenced in controls via a variable in the form $<component>svc . In the example below, the selected kubelet unitfile is referenced with $kubeletsvc in the remediation of the check . id: 2.1.1 # ... remediation: | Edit the kubelet service file $kubeletsvc on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --allow-privileged=false Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service # ... kubeconfig : A list of candidate kubeconfig files for a component. kube-bench checks this list and selects the first file that is found on the node. If none of the files exists, kube-bench defaults kubeconfig to the value of defaultkubeconfig . The selected kubeconfig for a component can be referenced in controls with a variable in the form $<component>kubeconfig . In the example below, the selected kubelet kubeconfig is referenced with $kubeletkubeconfig in the audit command. id: 2.2.1 text: \"Ensure that the kubelet.conf file permissions are set to 644 or more restrictive (Scored)\" audit: \"/bin/sh -c 'if test -e $kubeletkubeconfig; then stat -c %a $kubeletkubeconfig; fi'\" # ...","title":"Test and config files"},{"location":"controls/#test-and-config-files","text":"kube-bench runs checks specified in controls files that are a YAML representation of the CIS Kubernetes Benchmark checks (or other distribution-specific hardening guides).","title":"Test and config files"},{"location":"controls/#controls","text":"controls is a YAML document that contains checks that must be run against a specific Kubernetes node type, master or node and version. controls is the fundamental input to kube-bench . The following is an example of a basic controls : --- controls: id: 1 text: \"Master Node Security Configuration\" type: \"master\" groups: - id: 1.1 text: API Server checks: - id: 1.1.1 text: \"Ensure that the --allow-privileged argument is set (Scored)\" audit: \"ps -ef | grep kube-apiserver | grep -v grep\" tests: bin_op: or test_items: - flag: \"--allow-privileged\" set: true - flag: \"--some-other-flag\" set: false remediation: \"Edit the /etc/kubernetes/config file on the master node and set the KUBE_ALLOW_PRIV parameter to '--allow-privileged=false'\" scored: true - id: 1.2 text: Scheduler checks: - id: 1.2.1 text: \"Ensure that the --profiling argument is set to false (Scored)\" audit: \"ps -ef | grep kube-scheduler | grep -v grep\" tests: bin_op: and test_items: - flag: \"--profiling\" set: true - flag: \"--some-other-flag\" set: false remediation: \"Edit the /etc/kubernetes/config file on the master node and set the KUBE_ALLOW_PRIV parameter to '--allow-privileged=false'\" scored: true controls is composed of a hierarchy of groups, sub-groups and checks. Each of the controls components have an id and a text description which are displayed in the kube-bench output. type specifies what kubernetes node type a controls is for. Possible values for type are master and node .","title":"Controls"},{"location":"controls/#groups","text":"groups is a list of subgroups that test the various Kubernetes components that run on the node type specified in the controls . For example, one subgroup checks parameters passed to the API server binary, while another subgroup checks parameters passed to the controller-manager binary. groups: - id: 1.1 text: API Server # ... - id: 1.2 text: Scheduler # ... These subgroups have id , text fields which serve the same purposes described in the previous paragraphs. The most important part of the subgroup is the checks field which is the collection of actual check s that form the subgroup. This is an example of a subgroup and checks in the subgroup. id: 1.1 text: API Server checks: - id: 1.1.1 text: \"Ensure that the --allow-privileged argument is set (Scored)\" audit: \"ps -ef | grep kube-apiserver | grep -v grep\" tests: # ... - id: 1.1.2 text: \"Ensure that the --anonymous-auth argument is set to false (Not Scored)\" audit: \"ps -ef | grep kube-apiserver | grep -v grep\" tests: # ... kube-bench supports running a subgroup by specifying the subgroup id on the command line, with the flag --group or -g .","title":"Groups"},{"location":"controls/#check","text":"The CIS Kubernetes Benchmark recommends configurations to harden Kubernetes components. These recommendations are usually configuration options and can be specified by flags to Kubernetes binaries, or in configuration files. The Benchmark also provides commands to audit a Kubernetes installation, identify places where the cluster security can be improved, and steps to remediate these identified problems. In kube-bench , check objects embody these recommendations. This an example check object: id: 1.1.1 text: \"Ensure that the --anonymous-auth argument is set to false (Not Scored)\" audit: \"ps -ef | grep kube-apiserver | grep -v grep\" tests: test_items: - flag: \"--anonymous-auth\" compare: op: eq value: false set: true remediation: | Edit the API server pod specification file kube-apiserver on the master node and set the below parameter. --anonymous-auth=false scored: false A check object has an id , a text , an audit , a tests , remediation and scored fields. kube-bench supports running individual checks by specifying the check's id as a comma-delimited list on the command line with the --check flag. The audit field specifies the command to run for a check. The output of this command is then evaluated for conformance with the CIS Kubernetes Benchmark recommendation. The audit is evaluated against criteria specified by the tests object. tests contain bin_op and test_items . test_items specify the criteria(s) the audit command's output should meet to pass a check. This criteria is made up of keywords extracted from the output of the audit command and operations that compare these keywords against values expected by the CIS Kubernetes Benchmark. There are three ways to run and extract keywords from the output of the command used, | Command | Output var | |---|---| | audit | flag | | audit_config | path | | audit_env | env | flag is used when the keyword is a command-line flag. The associated audit command could be any binaries available on the system like ps command and a grep for the binary whose flag we are checking: ps -ef | grep somebinary | grep -v grep Here is an example usage of the flag option: # ... audit: \"ps -ef | grep kube-apiserver | grep -v grep\" tests: test_items: - flag: \"--anonymous-auth\" # ... path is used when the keyword is an option set in a JSON or YAML config file. The associated audit_command command is usually cat /path/to/config-yaml-or-json . For example: # ... text: \"Ensure that the --anonymous-auth argument is set to false (Not Scored)\" audit: \"cat /path/to/some/config\" tests: test_items: - path: \"{.someoption.value}\" # ... env is used to check if the value is present within a specified environment variable. The presence of env is treated as an OR operation, if both flag and env are supplied it will use either to attempt pass the check. The command used for checking the environment variables of a process is generated by default . If the command being generated is causing errors, you can override the command used by setting audit_env on the check. Similarly, if you don't want the environment checking command to be generated or run at all, specify disableEnvTesting as true on the check. The example below will check if the flag --auto-tls is equal to false OR ETCD_AUTO_TLS is equal to false test_items: - flag: \"--auto-tls\" env: \"ETCD_AUTO_TLS\" compare: op: eq value: false Note: flag, path and env will act as OR if more then one present. test_item compares the output of the audit command and keywords using the set and compare fields. test_items: - flag: \"--anonymous-auth\" compare: op: eq value: false set: true set checks if a keyword is present in the output of the audit command or a config file. The possible values for set are true and false. If set is true, the check passes only if the keyword is present in the output of the audit command, or config file. If set is false, the check passes only if the keyword is not present in the output of the audit command, or config file. set is true by default. compare has two fields op and value to compare keywords with expected value. op specifies which operation is used for the comparison, and value specifies the value to compare against. To use compare , set must true. The comparison will be ignored if set is false The op (operations) currently supported in kube-bench are: - eq : tests if the keyword is equal to the compared value. - noteq : tests if the keyword is unequal to the compared value. - gt : tests if the keyword is greater than the compared value. - gte : tests if the keyword is greater than or equal to the compared value. - lt : tests if the keyword is less than the compared value. - lte : tests if the keyword is less than or equal to the compared value. - has : tests if the keyword contains the compared value. - nothave : tests if the keyword does not contain the compared value. - regex : tests if the flag value matches the compared value regular expression. When defining regular expressions in YAML it is generally easier to wrap them in single quotes, for example '^[abc]$' , to avoid issues with string escaping. - bitmask : tests if keyward is bitmasked with the compared value, common usege is for comparing file permissions in linux.","title":"Check"},{"location":"controls/#omitting-checks","text":"If you decide that a recommendation is not appropriate for your environment, you can choose to omit it by editing the test YAML file to give it the check type skip as in this example: checks : - id : 2.1.1 text : \"Ensure that the --allow-privileged argument is set to false (Scored)\" type : \"skip\" scored : true No tests will be run for this check and the output will be marked [INFO].","title":"Omitting checks"},{"location":"controls/#configuration-and-variables","text":"Kubernetes component configuration and binary file locations and names vary based on cluster deployment methods and Kubernetes distribution used. For this reason, the locations of these binaries and config files are configurable by editing the cfg/config.yaml file and these binaries and files can be referenced in a controls file via variables. The cfg/config.yaml file is a global configuration file. Configuration files can be created for specific Kubernetes versions (distributions). Values in the version-specific config overwrite similar values in cfg/config.yaml . For example, the kube-apiserver in Red Hat OCP distribution is run as hypershift openshift-kube-apiserver instead of the default kube-apiserver . This difference can be specified by editing the master.apiserver.defaultbin entry cfg/rh-0.7/config.yaml . Below is the structure of cfg/config.yaml : nodetype |-- components |-- component1 |-- component1 |-- bins |-- defaultbin (optional) |-- confs |-- defaultconf (optional) |-- svcs |-- defaultsvc (optional) |-- kubeconfig |-- defaultkubeconfig (optional) Every node type has a subsection that specifies the main configuration items. components : A list of components for the node type. For example master will have an entry for apiserver , scheduler and controllermanager . Each component has the following entries: bins : A list of candidate binaries for a component. kube-bench checks this list and selects the first binary that is running on the node. If none of the binaries in bins list is running, kube-bench checks if the binary specified by defaultbin is running and terminates if none of the binaries in both bins and defaultbin is running. The selected binary for a component can be referenced in controls using a variable in the form $<component>bin . In the example below, we reference the selected API server binary with the variable $apiserverbin in an audit command. id: 1.1.1 text: \"Ensure that the --anonymous-auth argument is set to false (Scored)\" audit: \"ps -ef | grep $apiserverbin | grep -v grep\" # ... confs : A list of candidate configuration files for a component. kube-bench checks this list and selects the first config file that is found on the node. If none of the config files exists, kube-bench defaults conf to the value of defaultconf . The selected config for a component can be referenced in controls using a variable in the form $<component>conf . In the example below, we reference the selected API server config file with the variable $apiserverconf in an audit command. id: 1.4.1 text: \"Ensure that the API server pod specification file permissions are set to 644 or more restrictive (Scored)\" audit: \"/bin/sh -c 'if test -e $apiserverconf; then stat -c %a $apiserverconf; fi'\" svcs : A list of candidate unitfiles for a component. kube-bench checks this list and selects the first unitfile that is found on the node. If none of the unitfiles exists, kube-bench defaults unitfile to the value of defaultsvc . The selected unitfile for a component can be referenced in controls via a variable in the form $<component>svc . In the example below, the selected kubelet unitfile is referenced with $kubeletsvc in the remediation of the check . id: 2.1.1 # ... remediation: | Edit the kubelet service file $kubeletsvc on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --allow-privileged=false Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service # ... kubeconfig : A list of candidate kubeconfig files for a component. kube-bench checks this list and selects the first file that is found on the node. If none of the files exists, kube-bench defaults kubeconfig to the value of defaultkubeconfig . The selected kubeconfig for a component can be referenced in controls with a variable in the form $<component>kubeconfig . In the example below, the selected kubelet kubeconfig is referenced with $kubeletkubeconfig in the audit command. id: 2.2.1 text: \"Ensure that the kubelet.conf file permissions are set to 644 or more restrictive (Scored)\" audit: \"/bin/sh -c 'if test -e $kubeletkubeconfig; then stat -c %a $kubeletkubeconfig; fi'\" # ...","title":"Configuration and Variables"},{"location":"flags-and-commands/","text":"Commands Command Description help Prints help about any command run List of components to run version Print kube-bench version Flags Flag Description --alsologtostderr log to standard error as well as files --asff Send findings to AWS Security Hub for any benchmark tests that fail or that generate a warning. See [this page][kube-bench-aws-security-hub] for more information on how to enable the kube-bench integration with AWS Security Hub. --benchmark Manually specify CIS benchmark version -c, --check A comma-delimited list of checks to run as specified in Benchmark document. --config config file (default is ./cfg/config.yaml) --exit-code Specify the exit code for when checks fail --group Run all the checks under this comma-delimited list of groups. --include-test-output Prints the actual result when test fails. --json Prints the results as JSON --junit Prints the results as JUnit --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --logtostderr log to standard error instead of files --noremediations Disable printing of remediations section to stdout. --noresults Disable printing of results section to stdout. --nototals Disable calculating and printing of totals for failed, passed, ... checks across all sections --outputfile Writes the JSON results to output file --pgsql Save the results to PostgreSQL --scored Run the scored CIS checks (default true) --skip string List of comma separated values of checks to be skipped --stderrthreshold severity logs at or above this threshold go to stderr (default 2) -v, --v Level log level for V logs (default 0) --version string Manually specify Kubernetes version, automatically detected if unset --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging Examples Report kube-bench findings to AWS Security Hub You can configure kube-bench with the --asff option to send findings to AWS Security Hub for any benchmark tests that fail or that generate a warning. See this page for more information on how to enable the kube-bench integration with AWS Security Hub. Specifying the benchmark or Kubernetes version kube-bench uses the Kubernetes API, or access to the kubectl or kubelet executables to try to determine the Kubernetes version, and hence which benchmark to run. If you wish to override this, or if none of these methods are available, you can specify either the Kubernetes version or CIS Benchmark as a command line parameter. You can specify a particular version of Kubernetes by setting the --version flag or with the KUBE_BENCH_VERSION environment variable. The value of --version takes precedence over the value of KUBE_BENCH_VERSION . For example, run kube-bench using the tests for Kubernetes version 1.13: kube-bench --version 1.13 You can specify --benchmark to run a specific CIS Benchmark version: kube-bench --benchmark cis-1.5 Note: It is an error to specify both --version and --benchmark flags together Specifying Benchmark sections If you want to run specific CIS Benchmark sections (i.e master, node, etcd, etc...) you can use the run --targets subcommand. kube-bench run --targets master,node or kube-bench run --targets master,node,etcd,policies If no targets are specified, kube-bench will determine the appropriate targets based on the CIS Benchmark version and the components detected on the node. The detection is done by verifying which components are running, as defined in the config files (see Configuration . Run specific check or group kube-bench supports running individual checks by specifying the check's id as a comma-delimited list on the command line with the --check | -c flag. kube-bench --check=\"1.1.1,1.1.2,1.2.1,1.3.3\" kube-bench supports running all checks under group by specifying the group's id as a comma-delimited list on the command line with the --group | -g flag. kube-bench --check=\"1.1,2.2\" Will run all checks 1.1.X and 2.2.X. Skip specific check or group kube-bench supports skipping checks or groups by specifying the id as a comma-delimited list on the command line with the --skip flag. kube-bench --skip=\"1.1,1.2.1,1.3.3\" Will skip 1.1.X group and individual checks 1.2.1, 1.3.3. Skipped checks returns [INFO] output. Exit code kube-bench supports using uniqe exit code when failing a check or more. kube-bench --exit-code 42 Will return 42 if one check or more failed, and 0 incase none failed. Note: [WARN] is not [FAIL]. Output manipulation flags There are four output states: - [PASS] indicates that the test was run successfully, and passed. - [FAIL] indicates that the test was run successfully, and failed. The remediation output describes how to correct the configuration, or includes an error message describing why the test could not be run. - [WARN] means this test needs further attention, for example it is a test that needs to be run manually. Check the remediation output for further information. - [INFO] is informational output that needs no further action. Note: - If the test is Manual, this always generates WARN (because the user has to run it manually) - If the test is Scored, and kube-bench was unable to run the test, this generates FAIL (because the test has not been passed, and as a Scored test, if it doesn't pass then it must be considered a failure). - If the test is Not Scored, and kube-bench was unable to run the test, this generates WARN. - If the test is Scored, type is empty, and there are no test_items present, it generates a WARN. This is to highlight tests that appear to be incompletely defined. kube-bench supports multiple output manipulation flags. kube-bench --include-test-output will print failing checks output in the results section [INFO] 1 Master Node Security Configuration [INFO] 1.1 Master Node Configuration Files [FAIL] 1.1.1 Ensure that the API server pod specification file permissions are set to 644 or more restrictive (Automated) **permissions=777** Note: --noresults --noremediations and --include-test-output will not effect the json output but only stdout. Only --nototals will effect the json output and thats because it will not call the function to calculate totals. Troubleshooting Running kube-bench with the -v 3 parameter will generate debug logs that can be very helpful for debugging problems. If you are using one of the example job*.yaml files, you will need to edit the command field, for example [\"kube-bench\", \"-v\", \"3\"] . Once the job has run, the logs can be retrieved using kubectl logs on the job's pod.","title":"Flags and commands"},{"location":"flags-and-commands/#commands","text":"Command Description help Prints help about any command run List of components to run version Print kube-bench version","title":"Commands"},{"location":"flags-and-commands/#flags","text":"Flag Description --alsologtostderr log to standard error as well as files --asff Send findings to AWS Security Hub for any benchmark tests that fail or that generate a warning. See [this page][kube-bench-aws-security-hub] for more information on how to enable the kube-bench integration with AWS Security Hub. --benchmark Manually specify CIS benchmark version -c, --check A comma-delimited list of checks to run as specified in Benchmark document. --config config file (default is ./cfg/config.yaml) --exit-code Specify the exit code for when checks fail --group Run all the checks under this comma-delimited list of groups. --include-test-output Prints the actual result when test fails. --json Prints the results as JSON --junit Prints the results as JUnit --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --logtostderr log to standard error instead of files --noremediations Disable printing of remediations section to stdout. --noresults Disable printing of results section to stdout. --nototals Disable calculating and printing of totals for failed, passed, ... checks across all sections --outputfile Writes the JSON results to output file --pgsql Save the results to PostgreSQL --scored Run the scored CIS checks (default true) --skip string List of comma separated values of checks to be skipped --stderrthreshold severity logs at or above this threshold go to stderr (default 2) -v, --v Level log level for V logs (default 0) --version string Manually specify Kubernetes version, automatically detected if unset --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Flags"},{"location":"flags-and-commands/#examples","text":"","title":"Examples"},{"location":"flags-and-commands/#report-kube-bench-findings-to-aws-security-hub","text":"You can configure kube-bench with the --asff option to send findings to AWS Security Hub for any benchmark tests that fail or that generate a warning. See this page for more information on how to enable the kube-bench integration with AWS Security Hub.","title":"Report kube-bench findings to AWS Security Hub"},{"location":"flags-and-commands/#specifying-the-benchmark-or-kubernetes-version","text":"kube-bench uses the Kubernetes API, or access to the kubectl or kubelet executables to try to determine the Kubernetes version, and hence which benchmark to run. If you wish to override this, or if none of these methods are available, you can specify either the Kubernetes version or CIS Benchmark as a command line parameter. You can specify a particular version of Kubernetes by setting the --version flag or with the KUBE_BENCH_VERSION environment variable. The value of --version takes precedence over the value of KUBE_BENCH_VERSION . For example, run kube-bench using the tests for Kubernetes version 1.13: kube-bench --version 1.13 You can specify --benchmark to run a specific CIS Benchmark version: kube-bench --benchmark cis-1.5 Note: It is an error to specify both --version and --benchmark flags together","title":"Specifying the benchmark or Kubernetes version"},{"location":"flags-and-commands/#specifying-benchmark-sections","text":"If you want to run specific CIS Benchmark sections (i.e master, node, etcd, etc...) you can use the run --targets subcommand. kube-bench run --targets master,node or kube-bench run --targets master,node,etcd,policies If no targets are specified, kube-bench will determine the appropriate targets based on the CIS Benchmark version and the components detected on the node. The detection is done by verifying which components are running, as defined in the config files (see Configuration .","title":"Specifying Benchmark sections"},{"location":"flags-and-commands/#run-specific-check-or-group","text":"kube-bench supports running individual checks by specifying the check's id as a comma-delimited list on the command line with the --check | -c flag. kube-bench --check=\"1.1.1,1.1.2,1.2.1,1.3.3\" kube-bench supports running all checks under group by specifying the group's id as a comma-delimited list on the command line with the --group | -g flag. kube-bench --check=\"1.1,2.2\" Will run all checks 1.1.X and 2.2.X.","title":"Run specific check or group"},{"location":"flags-and-commands/#skip-specific-check-or-group","text":"kube-bench supports skipping checks or groups by specifying the id as a comma-delimited list on the command line with the --skip flag. kube-bench --skip=\"1.1,1.2.1,1.3.3\" Will skip 1.1.X group and individual checks 1.2.1, 1.3.3. Skipped checks returns [INFO] output.","title":"Skip specific check or group"},{"location":"flags-and-commands/#exit-code","text":"kube-bench supports using uniqe exit code when failing a check or more. kube-bench --exit-code 42 Will return 42 if one check or more failed, and 0 incase none failed. Note: [WARN] is not [FAIL].","title":"Exit code"},{"location":"flags-and-commands/#output-manipulation-flags","text":"There are four output states: - [PASS] indicates that the test was run successfully, and passed. - [FAIL] indicates that the test was run successfully, and failed. The remediation output describes how to correct the configuration, or includes an error message describing why the test could not be run. - [WARN] means this test needs further attention, for example it is a test that needs to be run manually. Check the remediation output for further information. - [INFO] is informational output that needs no further action. Note: - If the test is Manual, this always generates WARN (because the user has to run it manually) - If the test is Scored, and kube-bench was unable to run the test, this generates FAIL (because the test has not been passed, and as a Scored test, if it doesn't pass then it must be considered a failure). - If the test is Not Scored, and kube-bench was unable to run the test, this generates WARN. - If the test is Scored, type is empty, and there are no test_items present, it generates a WARN. This is to highlight tests that appear to be incompletely defined. kube-bench supports multiple output manipulation flags. kube-bench --include-test-output will print failing checks output in the results section [INFO] 1 Master Node Security Configuration [INFO] 1.1 Master Node Configuration Files [FAIL] 1.1.1 Ensure that the API server pod specification file permissions are set to 644 or more restrictive (Automated) **permissions=777** Note: --noresults --noremediations and --include-test-output will not effect the json output but only stdout. Only --nototals will effect the json output and thats because it will not call the function to calculate totals.","title":"Output manipulation flags"},{"location":"flags-and-commands/#troubleshooting","text":"Running kube-bench with the -v 3 parameter will generate debug logs that can be very helpful for debugging problems. If you are using one of the example job*.yaml files, you will need to edit the command field, for example [\"kube-bench\", \"-v\", \"3\"] . Once the job has run, the logs can be retrieved using kubectl logs on the job's pod.","title":"Troubleshooting"},{"location":"installation/","text":"Installation You can choose to * Run kube-bench from inside a container (sharing PID namespace with the host). See Running inside a container for additional details. * Run a container that installs kube-bench on the host, and then run kube-bench directly on the host. See Installing from a container for additional details. * install the latest binaries from the Releases page , though please note that you also need to download the config and test files from the cfg directory. See Download and Install binaries for details. * Compile it from source. See Installing from sources for details. Download and Install binaries It is possible to manually install and run kube-bench release binaries. In order to do that, you must have access to your Kubernetes cluster nodes. Note that if you're using one of the managed Kubernetes services (e.g. EKS, AKS, GKE, ACK, OCP), you will not have access to the master nodes of your cluster and you can\u2019t perform any tests on the master nodes. First, log into one of the nodes using SSH. Install kube-bench binary for your platform using the commands below. Note that there may be newer releases available. See releases page . Ubuntu/Debian: curl -L https://github.com/aquasecurity/kube-bench/releases/download/v0.6.2/kube-bench_0.6.2_linux_amd64.deb -o kube-bench_0.6.2_linux_amd64.deb sudo apt install ./kube-bench_0.6.2_linux_amd64.deb -f RHEL: curl -L https://github.com/aquasecurity/kube-bench/releases/download/v0.6.2/kube-bench_0.6.2_linux_amd64.rpm -o kube-bench_0.6.2_linux_amd64.rpm sudo yum install kube-bench_0.6.2_linux_amd64.rpm -y Alternatively, you can manually download and extract the kube-bench binary: curl -L https://github.com/aquasecurity/kube-bench/releases/download/v0.6.2/kube-bench_0.6.2_linux_amd64.tar.gz -o kube-bench_0.6.2_linux_amd64.tar.gz tar -xvf kube-bench_0.6.2_linux_amd64.tar.gz You can then run kube-bench directly: kube-bench If you manually downloaded the kube-bench binary (using curl command above), you have to specify the location of configuration directory and file. For example: ./kube-bench --config-dir `pwd`/cfg --config `pwd`/cfg/config.yaml See previous section on Running kube-bench for further details on using the kube-bench binary. Installing from sources If Go is installed on the target machines, you can simply clone this repository and run as follows (assuming your GOPATH is set ): go get github.com/aquasecurity/kube-bench cd $GOPATH /src/github.com/aquasecurity/kube-bench go build -o kube-bench . # See all supported options ./kube-bench --help # Run all checks ./kube-bench Installing from a container This command copies the kube-bench binary and configuration files to your host from the Docker container: binaries compiled for linux-x86-64 only (so they won't run on macOS or Windows) docker run --rm -v `pwd`:/host aquasec/kube-bench:latest install You can then run ./kube-bench .","title":"Installation"},{"location":"installation/#installation","text":"You can choose to * Run kube-bench from inside a container (sharing PID namespace with the host). See Running inside a container for additional details. * Run a container that installs kube-bench on the host, and then run kube-bench directly on the host. See Installing from a container for additional details. * install the latest binaries from the Releases page , though please note that you also need to download the config and test files from the cfg directory. See Download and Install binaries for details. * Compile it from source. See Installing from sources for details.","title":"Installation"},{"location":"installation/#download-and-install-binaries","text":"It is possible to manually install and run kube-bench release binaries. In order to do that, you must have access to your Kubernetes cluster nodes. Note that if you're using one of the managed Kubernetes services (e.g. EKS, AKS, GKE, ACK, OCP), you will not have access to the master nodes of your cluster and you can\u2019t perform any tests on the master nodes. First, log into one of the nodes using SSH. Install kube-bench binary for your platform using the commands below. Note that there may be newer releases available. See releases page . Ubuntu/Debian: curl -L https://github.com/aquasecurity/kube-bench/releases/download/v0.6.2/kube-bench_0.6.2_linux_amd64.deb -o kube-bench_0.6.2_linux_amd64.deb sudo apt install ./kube-bench_0.6.2_linux_amd64.deb -f RHEL: curl -L https://github.com/aquasecurity/kube-bench/releases/download/v0.6.2/kube-bench_0.6.2_linux_amd64.rpm -o kube-bench_0.6.2_linux_amd64.rpm sudo yum install kube-bench_0.6.2_linux_amd64.rpm -y Alternatively, you can manually download and extract the kube-bench binary: curl -L https://github.com/aquasecurity/kube-bench/releases/download/v0.6.2/kube-bench_0.6.2_linux_amd64.tar.gz -o kube-bench_0.6.2_linux_amd64.tar.gz tar -xvf kube-bench_0.6.2_linux_amd64.tar.gz You can then run kube-bench directly: kube-bench If you manually downloaded the kube-bench binary (using curl command above), you have to specify the location of configuration directory and file. For example: ./kube-bench --config-dir `pwd`/cfg --config `pwd`/cfg/config.yaml See previous section on Running kube-bench for further details on using the kube-bench binary.","title":"Download and Install binaries"},{"location":"installation/#installing-from-sources","text":"If Go is installed on the target machines, you can simply clone this repository and run as follows (assuming your GOPATH is set ): go get github.com/aquasecurity/kube-bench cd $GOPATH /src/github.com/aquasecurity/kube-bench go build -o kube-bench . # See all supported options ./kube-bench --help # Run all checks ./kube-bench","title":"Installing from sources"},{"location":"installation/#installing-from-a-container","text":"This command copies the kube-bench binary and configuration files to your host from the Docker container: binaries compiled for linux-x86-64 only (so they won't run on macOS or Windows) docker run --rm -v `pwd`:/host aquasec/kube-bench:latest install You can then run ./kube-bench .","title":"Installing from a container"},{"location":"platforms/","text":"CIS Kubernetes Benchmark support kube-bench supports running tests for Kubernetes. Most of our supported benchmarks are defined in the CIS Kubernetes Benchmarks . Some defined by other hardenening guides. Source Kubernetes Benchmark kube-bench config Kubernetes versions CIS 1.5.1 cis-1.5 1.15- CIS 1.6.0 cis-1.6 1.16- CIS GKE 1.0.0 gke-1.0 GKE CIS EKS 1.0.0 eks-1.0 EKS CIS ACK 1.0.0 ack-1.0 ACK RHEL RedHat OpenShift hardening guide rh-0.7 OCP 3.10-3.11 CIS OCP4 1.1.0 rh-1.0 OCP 4.1-","title":"Platforms"},{"location":"platforms/#cis-kubernetes-benchmark-support","text":"kube-bench supports running tests for Kubernetes. Most of our supported benchmarks are defined in the CIS Kubernetes Benchmarks . Some defined by other hardenening guides. Source Kubernetes Benchmark kube-bench config Kubernetes versions CIS 1.5.1 cis-1.5 1.15- CIS 1.6.0 cis-1.6 1.16- CIS GKE 1.0.0 gke-1.0 GKE CIS EKS 1.0.0 eks-1.0 EKS CIS ACK 1.0.0 ack-1.0 ACK RHEL RedHat OpenShift hardening guide rh-0.7 OCP 3.10-3.11 CIS OCP4 1.1.0 rh-1.0 OCP 4.1-","title":"CIS Kubernetes Benchmark support"},{"location":"running/","text":"Running kube-bench If you run kube-bench directly from the command line you may need to be root / sudo to have access to all the config files. By default kube-bench attempts to auto-detect the running version of Kubernetes, and map this to the corresponding CIS Benchmark version. For example, Kubernetes version 1.15 is mapped to CIS Benchmark version cis-1.15 which is the benchmark version valid for Kubernetes 1.15. kube-bench also attempts to identify the components running on the node, and uses this to determine which tests to run (for example, only running the master node tests if the node is running an API server). Please note It is impossible to inspect the master nodes of managed clusters, e.g. GKE, EKS, AKS and ACK, using kube-bench as one does not have access to such nodes, although it is still possible to use kube-bench to check worker node configuration in these environments. Running inside a container You can avoid installing kube-bench on the host by running it inside a container using the host PID namespace and mounting the /etc and /var directories where the configuration and other files are located on the host so that kube-bench can check their existence and permissions. docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -t aquasec/kube-bench:latest --version 1.18 Note: the tests require either the kubelet or kubectl binary in the path in order to auto-detect the Kubernetes version. You can pass -v $(which kubectl):/usr/local/mount-from-host/bin/kubectl to resolve this. You will also need to pass in kubeconfig credentials. For example: docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -v $(which kubectl):/usr/local/mount-from-host/bin/kubectl -v ~/.kube:/.kube -e KUBECONFIG=/.kube/config -t aquasec/kube-bench:latest You can use your own configs by mounting them over the default ones in /opt/kube-bench/cfg/ docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -t -v path/to/my-config.yaml:/opt/kube-bench/cfg/config.yaml -v $(which kubectl):/usr/local/mount-from-host/bin/kubectl -v ~/.kube:/.kube -e KUBECONFIG=/.kube/config aquasec/kube-bench:latest Running in a Kubernetes cluster You can run kube-bench inside a pod, but it will need access to the host's PID namespace in order to check the running processes, as well as access to some directories on the host where config files and other files are stored. The supplied job.yaml file can be applied to run the tests as a job. For example: $ kubectl apply -f job.yaml job.batch/kube-bench created $ kubectl get pods NAME READY STATUS RESTARTS AGE kube-bench-j76s9 0 /1 ContainerCreating 0 3s # Wait for a few seconds for the job to complete $ kubectl get pods NAME READY STATUS RESTARTS AGE kube-bench-j76s9 0 /1 Completed 0 11s # The results are held in the pod's logs kubectl logs kube-bench-j76s9 [ INFO ] 1 Master Node Security Configuration [ INFO ] 1 .1 API Server ... To run tests on the master node, the pod needs to be scheduled on that node. This involves setting a nodeSelector and tolerations in the pod spec. The default labels applied to master nodes has changed since Kubernetes 1.11, so if you are using an older version you may need to modify the nodeSelector and tolerations to run the job on the master node. Running in an AKS cluster Create an AKS cluster(e.g. 1.13.7) with RBAC enabled, otherwise there would be 4 failures Use the kubectl-enter plugin to shell into a node kubectl-enter {node-name} or ssh to one agent node could open nsg 22 port and assign a public ip for one agent node (only for testing purpose) Run CIS benchmark to view results: docker run --rm -v `pwd`:/host aquasec/kube-bench:latest install ./kube-bench kube-bench cannot be run on AKS master nodes Running in an EKS cluster There is a job-eks.yaml file for running the kube-bench node checks on an EKS cluster. The significant difference on EKS is that it's not possible to schedule jobs onto the master node, so master checks can't be performed To create an EKS Cluster refer to Getting Started with Amazon EKS in the Amazon EKS User Guide Information on configuring eksctl , kubectl and the AWS CLI is within Create an Amazon Elastic Container Registry (ECR) repository to host the kube-bench container image aws ecr create-repository --repository-name k8s/kube-bench --image-tag-mutability MUTABLE Download, build and push the kube-bench container image to your ECR repo git clone https://github.com/aquasecurity/kube-bench.git cd kube-bench aws ecr get-login-password --region <AWS_REGION> | docker login --username AWS --password-stdin <AWS_ACCT_NUMBER>.dkr.ecr.<AWS_REGION>.amazonaws.com docker build -t k8s/kube-bench . docker tag k8s/kube-bench:latest <AWS_ACCT_NUMBER>.dkr.ecr.<AWS_REGION>.amazonaws.com/k8s/kube-bench:latest docker push <AWS_ACCT_NUMBER>.dkr.ecr.<AWS_REGION>.amazonaws.com/k8s/kube-bench:latest Copy the URI of your pushed image, the URI format is like this: <AWS_ACCT_NUMBER>.dkr.ecr.<AWS_REGION>.amazonaws.com/k8s/kube-bench:latest Replace the image value in job-eks.yaml with the URI from Step 4 Run the kube-bench job on a Pod in your Cluster: kubectl apply -f job-eks.yaml Find the Pod that was created, it should be in the default namespace: kubectl get pods --all-namespaces Retrieve the value of this Pod and output the report, note the Pod name will vary: kubectl logs kube-bench-<value> You can save the report for later reference: kubectl logs kube-bench-<value> > kube-bench-report.txt Running on OpenShift OpenShift Hardening Guide kube-bench config ocp-3.10 + rh-0.7 ocp-4.1 + rh-1.0 kube-bench includes a set of test files for Red Hat's OpenShift hardening guide for OCP 3.10 and 4.1. To run this you will need to specify --benchmark rh-07 , or --version ocp-3.10 or, --version ocp-4.5 or --benchmark rh-1.0 kube-bench supports auto-detection, when you run the kube-bench command it will autodetect if running in openshift environment. Running in a GKE cluster CIS Benchmark Targets gke-1.0 master, controlplane, node, etcd, policies, managedservices kube-bench includes benchmarks for GKE. To run this you will need to specify --benchmark gke-1.0 when you run the kube-bench command. To run the benchmark as a job in your GKE cluster apply the included job-gke.yaml . kubectl apply -f job-gke.yaml Running in a ACK cluster CIS Benchmark Targets ack-1.0 master, controlplane, node, etcd, policies, managedservices kube-bench includes benchmarks for Alibaba Cloud Container Service For Kubernetes (ACK). To run this you will need to specify --benchmark ack-1.0 when you run the kube-bench command. To run the benchmark as a job in your ACK cluster apply the included job-ack.yaml . kubectl apply -f job-ack.yaml","title":"Running"},{"location":"running/#running-kube-bench","text":"If you run kube-bench directly from the command line you may need to be root / sudo to have access to all the config files. By default kube-bench attempts to auto-detect the running version of Kubernetes, and map this to the corresponding CIS Benchmark version. For example, Kubernetes version 1.15 is mapped to CIS Benchmark version cis-1.15 which is the benchmark version valid for Kubernetes 1.15. kube-bench also attempts to identify the components running on the node, and uses this to determine which tests to run (for example, only running the master node tests if the node is running an API server). Please note It is impossible to inspect the master nodes of managed clusters, e.g. GKE, EKS, AKS and ACK, using kube-bench as one does not have access to such nodes, although it is still possible to use kube-bench to check worker node configuration in these environments.","title":"Running kube-bench"},{"location":"running/#running-inside-a-container","text":"You can avoid installing kube-bench on the host by running it inside a container using the host PID namespace and mounting the /etc and /var directories where the configuration and other files are located on the host so that kube-bench can check their existence and permissions. docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -t aquasec/kube-bench:latest --version 1.18 Note: the tests require either the kubelet or kubectl binary in the path in order to auto-detect the Kubernetes version. You can pass -v $(which kubectl):/usr/local/mount-from-host/bin/kubectl to resolve this. You will also need to pass in kubeconfig credentials. For example: docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -v $(which kubectl):/usr/local/mount-from-host/bin/kubectl -v ~/.kube:/.kube -e KUBECONFIG=/.kube/config -t aquasec/kube-bench:latest You can use your own configs by mounting them over the default ones in /opt/kube-bench/cfg/ docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -t -v path/to/my-config.yaml:/opt/kube-bench/cfg/config.yaml -v $(which kubectl):/usr/local/mount-from-host/bin/kubectl -v ~/.kube:/.kube -e KUBECONFIG=/.kube/config aquasec/kube-bench:latest","title":"Running inside a container"},{"location":"running/#running-in-a-kubernetes-cluster","text":"You can run kube-bench inside a pod, but it will need access to the host's PID namespace in order to check the running processes, as well as access to some directories on the host where config files and other files are stored. The supplied job.yaml file can be applied to run the tests as a job. For example: $ kubectl apply -f job.yaml job.batch/kube-bench created $ kubectl get pods NAME READY STATUS RESTARTS AGE kube-bench-j76s9 0 /1 ContainerCreating 0 3s # Wait for a few seconds for the job to complete $ kubectl get pods NAME READY STATUS RESTARTS AGE kube-bench-j76s9 0 /1 Completed 0 11s # The results are held in the pod's logs kubectl logs kube-bench-j76s9 [ INFO ] 1 Master Node Security Configuration [ INFO ] 1 .1 API Server ... To run tests on the master node, the pod needs to be scheduled on that node. This involves setting a nodeSelector and tolerations in the pod spec. The default labels applied to master nodes has changed since Kubernetes 1.11, so if you are using an older version you may need to modify the nodeSelector and tolerations to run the job on the master node.","title":"Running in a Kubernetes cluster"},{"location":"running/#running-in-an-aks-cluster","text":"Create an AKS cluster(e.g. 1.13.7) with RBAC enabled, otherwise there would be 4 failures Use the kubectl-enter plugin to shell into a node kubectl-enter {node-name} or ssh to one agent node could open nsg 22 port and assign a public ip for one agent node (only for testing purpose) Run CIS benchmark to view results: docker run --rm -v `pwd`:/host aquasec/kube-bench:latest install ./kube-bench kube-bench cannot be run on AKS master nodes","title":"Running in an AKS cluster"},{"location":"running/#running-in-an-eks-cluster","text":"There is a job-eks.yaml file for running the kube-bench node checks on an EKS cluster. The significant difference on EKS is that it's not possible to schedule jobs onto the master node, so master checks can't be performed To create an EKS Cluster refer to Getting Started with Amazon EKS in the Amazon EKS User Guide Information on configuring eksctl , kubectl and the AWS CLI is within Create an Amazon Elastic Container Registry (ECR) repository to host the kube-bench container image aws ecr create-repository --repository-name k8s/kube-bench --image-tag-mutability MUTABLE Download, build and push the kube-bench container image to your ECR repo git clone https://github.com/aquasecurity/kube-bench.git cd kube-bench aws ecr get-login-password --region <AWS_REGION> | docker login --username AWS --password-stdin <AWS_ACCT_NUMBER>.dkr.ecr.<AWS_REGION>.amazonaws.com docker build -t k8s/kube-bench . docker tag k8s/kube-bench:latest <AWS_ACCT_NUMBER>.dkr.ecr.<AWS_REGION>.amazonaws.com/k8s/kube-bench:latest docker push <AWS_ACCT_NUMBER>.dkr.ecr.<AWS_REGION>.amazonaws.com/k8s/kube-bench:latest Copy the URI of your pushed image, the URI format is like this: <AWS_ACCT_NUMBER>.dkr.ecr.<AWS_REGION>.amazonaws.com/k8s/kube-bench:latest Replace the image value in job-eks.yaml with the URI from Step 4 Run the kube-bench job on a Pod in your Cluster: kubectl apply -f job-eks.yaml Find the Pod that was created, it should be in the default namespace: kubectl get pods --all-namespaces Retrieve the value of this Pod and output the report, note the Pod name will vary: kubectl logs kube-bench-<value> You can save the report for later reference: kubectl logs kube-bench-<value> > kube-bench-report.txt","title":"Running in an EKS cluster"},{"location":"running/#running-on-openshift","text":"OpenShift Hardening Guide kube-bench config ocp-3.10 + rh-0.7 ocp-4.1 + rh-1.0 kube-bench includes a set of test files for Red Hat's OpenShift hardening guide for OCP 3.10 and 4.1. To run this you will need to specify --benchmark rh-07 , or --version ocp-3.10 or, --version ocp-4.5 or --benchmark rh-1.0 kube-bench supports auto-detection, when you run the kube-bench command it will autodetect if running in openshift environment.","title":"Running on OpenShift"},{"location":"running/#running-in-a-gke-cluster","text":"CIS Benchmark Targets gke-1.0 master, controlplane, node, etcd, policies, managedservices kube-bench includes benchmarks for GKE. To run this you will need to specify --benchmark gke-1.0 when you run the kube-bench command. To run the benchmark as a job in your GKE cluster apply the included job-gke.yaml . kubectl apply -f job-gke.yaml","title":"Running in a GKE cluster"},{"location":"running/#running-in-a-ack-cluster","text":"CIS Benchmark Targets ack-1.0 master, controlplane, node, etcd, policies, managedservices kube-bench includes benchmarks for Alibaba Cloud Container Service For Kubernetes (ACK). To run this you will need to specify --benchmark ack-1.0 when you run the kube-bench command. To run the benchmark as a job in your ACK cluster apply the included job-ack.yaml . kubectl apply -f job-ack.yaml","title":"Running in a ACK cluster"}]}